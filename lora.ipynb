{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in order to obtain detailed metrics and results of our hyperparameter tuning, we utilise Weight&Biases (wandb), which is an external tool that requires user credentials. If you do not wish to use this tool, please make the necessary adjustements in the ```fine_tune``` function in the logging strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies to run the notebook \n",
    "# Uncomment below to install - may require additional installations depending on your python version\n",
    "\n",
    "# python version == 3.10.14\n",
    "# %pip install torch transformers peft datasets scikit-learn wandb accelerate -U --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, os, itertools, torch, wandb\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, accuracy_score\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from transformers import RobertaTokenizerFast, DataCollatorWithPadding, RobertaForSequenceClassification, \\\n",
    "                         TrainingArguments, Trainer, EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained model's name from HuggingFace\n",
    "PT_MODEL_NAME = 'roberta-large'\n",
    "\n",
    "# Fine-tuned model path (best model after hyperparameter tuning)\n",
    "FT_MODEL_PATH = './LoRA/Final-model'\n",
    "\n",
    "# Path to dataset in which all *.csv files are stored\n",
    "DATASET_PATH = './data'\n",
    "TRAIN_FILE_NAME, VAL_FILE_NAME = 'train.csv', 'dev.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 26944\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label'],\n",
       "        num_rows: 6737\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load training and validation datasets from csv files as a DatasetDict\n",
    "data_files = {\"train\": TRAIN_FILE_NAME, \"validation\": VAL_FILE_NAME}\n",
    "dataset = load_dataset(\"csv\", data_dir=DATASET_PATH, data_files=data_files)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model's tokenizer\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(PT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(samples):\n",
    "    ''' Replace None instances in hypothesis by empty strings. '''\n",
    "    samples['hypothesis'] = \"\" if samples['hypothesis'] is None else samples['hypothesis']\n",
    "    return samples\n",
    "\n",
    "# Preprocess texts (hypothesis) from the dataset\n",
    "dataset = dataset.map(text_preprocessing, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a2e2ed2c4e4f0e8a83ea89d9279517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26944 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67230e343244c3b82a2ba0e7a386904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 26944\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 6737\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_tokenization(samples):\n",
    "    ''' Tokenize the premise and hypothesis into sentence pair classification format. '''\n",
    "    return tokenizer(samples['premise'], samples['hypothesis'], truncation=True)\n",
    "\n",
    "# Tokenize dataset (premises and hypothesis) for sentence pair classification\n",
    "tokenized_dataset = dataset.map(text_tokenization, batched=True, remove_columns=['premise', 'hypothesis'])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a batch of samples using DataCollatorWithPadding. It is more efficient to dynamically pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA (Low-Rank Adaptation) preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init_lora():\n",
    "    ''' Function to load a pre-trained model on the most efficient available device,\n",
    "        and setup LoRA (Low-Rank Adaptation) to speed up training and lower computational costs.\n",
    "\n",
    "        LoRA is a technique designed to fine-tune very large language models by keeping the \n",
    "        pretrained parameters of the model frozen and introduce trainable low-rank matrices that adapt \n",
    "        the model's behavior for a specific task. This significantly reduces the number of trainable \n",
    "        parameters during fine-tuning, leading to faster training and reduced computational costs.\n",
    "        \n",
    "        In a typical transformer architecture, attention and feed-forward layers play crucial roles. \n",
    "        Therefore, LoRA specifically targets the weight matrices in these layers.\n",
    "\n",
    "        Publication: https://arxiv.org/abs/2106.09685 \n",
    "    '''\n",
    "    # Load pretrained model\n",
    "    pretrained_model = RobertaForSequenceClassification.from_pretrained(\n",
    "        PT_MODEL_NAME, num_labels=2, device_map='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    )\n",
    "    print('Device used:', 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Configure LoRA variation of the model by freezing layers and adding trainable low-rank matrices\n",
    "    peft_config = LoraConfig(task_type=\"SEQ_CLS\", inference_mode=False, r=8, lora_alpha=16, lora_dropout=0.1)\n",
    "    peft_model = get_peft_model(pretrained_model, peft_config)\n",
    "    print('LoRA parameters:', end=' '), peft_model.print_trainable_parameters()\n",
    "    return peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    ''' Function to calculate metrics given model predictions and expected labels. \n",
    "        Metrics: F1 (micro/macro/wei), Precision (macro), Recall (macro), ROC (macro), and Accuracy.\n",
    "        We use macro averages as the classes are imbalanced.\n",
    "    '''\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"F1_micro\": f1_score(labels, predictions, average='micro'),\n",
    "        \"F1_macro\": f1_score(labels, predictions, average='macro'),\n",
    "        \"F1_weighted\": f1_score(labels, predictions, average='weighted'),\n",
    "        \"Precision_macro\": precision_score(labels, predictions, average='macro'),\n",
    "        \"Recall_macro\": recall_score(labels, predictions, average='macro'),\n",
    "        \"ROC_macro\": roc_auc_score(labels, predictions, average = 'macro'),\n",
    "        \"Accuracy\": accuracy_score(labels, predictions),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(hyperparameters, dataset):\n",
    "  ''' Function to fine-tune a pretrained model using Hugging Face's pipeline. \n",
    "  '''\n",
    "  # Model's and logs directory\n",
    "  RUN_NAME = f\"Run - {datetime.datetime.now().strftime('%m-%d-%H-%M')}\"\n",
    "  DIR = f\"./LoRA/Results/{RUN_NAME}\"\n",
    "  os.environ[\"WANDB_PROJECT\"] = 'NLU-CWK' # set the wandb project where this run will be logged\n",
    "\n",
    "  # Define training args\n",
    "  training_args = TrainingArguments(\n",
    "    run_name=RUN_NAME.replace(' ', '_'),\n",
    "    output_dir=f\"{DIR}/checkpoints\",\n",
    "\n",
    "    # Parameters\n",
    "    per_device_train_batch_size = hyperparameters[\"batch_size\"],\n",
    "    per_device_eval_batch_size = hyperparameters[\"batch_size\"],\n",
    "    gradient_accumulation_steps = hyperparameters[\"gradient_accumulation_steps\"],\n",
    "    learning_rate = hyperparameters[\"learning_rate\"],\n",
    "    weight_decay = hyperparameters['weight_decay'],\n",
    "    num_train_epochs = 10,  # Use early stopping (so this is maximum epochs)\n",
    "    fp16 = True,            # Use 16-bit (mixed) precision instead of 32-bit (ONLY POSSIBLE ON CUDA!)\n",
    "    optim = \"adamw_torch\",\n",
    "    \n",
    "    use_cpu=False,\n",
    "    seed=42,                # Use a seed for reproducibility\n",
    "\n",
    "    # Logging\n",
    "    logging_dir=f\"{DIR}/training_logs\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    report_to=\"wandb\",\n",
    "    # Saving\n",
    "    save_strategy=\"epoch\",\n",
    "    # Evaluating (Use validation loss for model selection and early stopping)\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_safetensors=True, save_total_limit=1, load_best_model_at_end=True, \n",
    "    metric_for_best_model=\"eval_loss\", greater_is_better=False,\n",
    "  )\n",
    "\n",
    "  # Create a Trainer instance\n",
    "  trainer = Trainer(\n",
    "    model_init=model_init_lora,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['validation'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(2, 0.0)],\n",
    "  )\n",
    "\n",
    "  # Fine-tune the model\n",
    "  trainer.train()\n",
    "\n",
    "  # Evaluate the final model\n",
    "  evaluation_results = trainer.evaluate()\n",
    "\n",
    "  return evaluation_results, RUN_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters recommended by BERT and RoBERTa\n",
    "HYPERPARAMETERS = {\n",
    "    'weight_decay': [0.01, 0.02], \n",
    "    'learning_rate': [5e-5, 3e-5, 2e-5], \n",
    "    'gradient_accumulation_steps': [1, 2, 3], \n",
    "    'batch_size': [8], # 8, 16, 24 (with gradient accumulation)\n",
    "}\n",
    "\n",
    "# Get all possible combination of hyperparameter sets (for grid search)\n",
    "keys, values = zip(*HYPERPARAMETERS.items())\n",
    "HYPERPARAMETERS_COMB = [dict(zip(keys, v)) for v in itertools.product(*values)] # length = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and write logs during fine-tuning\n",
    "with open('LoRA/training_logs.txt', 'a+') as log:\n",
    "\n",
    "    # Fine-tune on each hyperparameter set (grid search)\n",
    "    for HYPERPARAM in HYPERPARAMETERS_COMB: # already trained one\n",
    "        print('--- STARTING FINE-TUNING ---')\n",
    "        print('Hyperparameters:', HYPERPARAM)\n",
    "        evaluation_results, run_name = fine_tune(hyperparameters=HYPERPARAM, dataset=tokenized_dataset)\n",
    "        print('Run name:', run_name)\n",
    "        print('Final evaluation results:', evaluation_results, '\\n')\n",
    "        \n",
    "        log.write(f\"Run name: {run_name}\\n\")\n",
    "        log.write(f\"Hyperparameters:\\n{', '.join([f'{key}={value}' for key, value in HYPERPARAM.items()])}.\\n\")\n",
    "        log.write(f\"Final evaluation results:{', '.join([f'{key}={value}' for key, value in evaluation_results.items()])}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model by hand based on validation loss and macro-F1 score\n",
    "# Best model: Run - 04-10-14-59\n",
    "best_model_dir = FT_MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "pretrained_model = RobertaForSequenceClassification.from_pretrained(\n",
    "    PT_MODEL_NAME, num_labels=2, device_map='cuda' if torch.cuda.is_available() else 'cpu',\n",
    ")\n",
    "\n",
    "# Add-in trained LoRA layers\n",
    "model = PeftModel.from_pretrained(\n",
    "    pretrained_model, best_model_dir, device_map='cuda' if torch.cuda.is_available() else 'cpu',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f404d418b70e426e83911f59d2b5ab28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbelkadisamuel\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/sam/Desktop/School/NLU/Coursework NLI/wandb/run-20240423_201803-up6l7qw1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/belkadisamuel/huggingface/runs/up6l7qw1/workspace' target=\"_blank\">summer-dew-2</a></strong> to <a href='https://wandb.ai/belkadisamuel/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/belkadisamuel/huggingface' target=\"_blank\">https://wandb.ai/belkadisamuel/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/belkadisamuel/huggingface/runs/up6l7qw1/workspace' target=\"_blank\">https://wandb.ai/belkadisamuel/huggingface/runs/up6l7qw1/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.22703786194324493,\n",
       " 'eval_F1_micro': 0.9159863440700609,\n",
       " 'eval_F1_macro': 0.9158481606898208,\n",
       " 'eval_F1_weighted': 0.9159590110937497,\n",
       " 'eval_Precision_macro': 0.9161779177848881,\n",
       " 'eval_Recall_macro': 0.9156366383815087,\n",
       " 'eval_ROC_macro': 0.9156366383815085,\n",
       " 'eval_Accuracy': 0.9159863440700609,\n",
       " 'eval_runtime': 154.0185,\n",
       " 'eval_samples_per_second': 43.741,\n",
       " 'eval_steps_per_second': 5.473}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the trained model and compute_metrics function to evaluate the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "eval_results = trainer.evaluate(tokenized_dataset['validation'])\n",
    "eval_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
